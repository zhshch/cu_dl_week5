{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Monet Painting GANs","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os, glob, random, shutil\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import EarlyStopping\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as dset\nfrom torchvision.utils import make_grid\nfrom torchvision.io import read_image\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2023-04-29T18:17:34.508826Z","iopub.execute_input":"2023-04-29T18:17:34.509610Z","iopub.status.idle":"2023-04-29T18:17:41.437408Z","shell.execute_reply.started":"2023-04-29T18:17:34.509569Z","shell.execute_reply":"2023-04-29T18:17:41.436199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"In this project, we will employ a GAN (Generative Adversarial Network) architecture, which comprises of two neural network models: the generator and the discriminator. These two models are designed to work in opposition to each other, with the generator attempting to deceive the discriminator by generating images that are similar to Monet's style, and the discriminator attempting to accurately distinguish between real Monet-style images and the generated ones. This adversarial process enables the generator to continuously improve its image generation capability until it finally produces highly-realistic Monet-style images that can be difficult to distinguish from genuine ones.\n\nThe dataset provided for this project consists of two main categories of images: Monet paintings and photographs. The Monet category includes a total of 300 high-quality painting images, each sized at 256x256 pixels, and provided in both JPEG and TFRecord formats (the same set of images). Similarly, the photograph category contains 7028 high-resolution photos, also sized at 256x256 pixels, offered in both JPEG and TFRecord formats. With this diverse set of images, our GAN architecture will be trained to learn to generate Monet-style images that closely resemble the paintings in the Monet directory using the real photographs provided in the photo directory.\nword count: 105, token ","metadata":{}},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"markdown","source":"We use `ImgTransform` to augment images before loading datasets. This introduces more variety during training and improves learning, especially with a limited number of Monet paintings. We apply basic transformations using `torchvision.transforms`, like `Resize`, `RandomHorizontalFlip`, and `RandomVerticalFlip`. These transformations are only necessary during model training, specified using the stage argument. Finally, we scale the images down for better convergence.","metadata":{}},{"cell_type":"code","source":"class ImgTransform:\n    def __init__(self, img_size=256):\n        self.train_transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomVerticalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5], std=[0.5])\n        ])\n        \n        self.test_transform = transforms.Compose([\n            transforms.Resize((img_size, img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5], std=[0.5])\n        ])\n        \n        \n    def __call__(self, img, stage=\"train\"):\n        if stage == \"train\":\n            img = self.train_transform(img)\n        else:\n            img = self.test_transform(img)\n        return img","metadata":{"execution":{"iopub.status.busy":"2023-04-29T18:17:45.306196Z","iopub.execute_input":"2023-04-29T18:17:45.306585Z","iopub.status.idle":"2023-04-29T18:17:45.315664Z","shell.execute_reply.started":"2023-04-29T18:17:45.306549Z","shell.execute_reply":"2023-04-29T18:17:45.313249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We create a custom `torch.utils.data.Dataset` class with `__init__`, `__len__`, and `__getitem__` methods to load and store datasets. \n\nWe use the stage argument to differentiate between training and prediction datasets, where the training dataset includes both Monet paintings and photos while the prediction dataset only includes photos.","metadata":{}},{"cell_type":"code","source":"class GANDataset(Dataset):\n    def __init__(self, photo_files, monet_files, transform, stage='train'):\n        self.photo_files = photo_files\n        self.monet_files = monet_files\n        self.transform = transform\n        self.stage = stage\n\n    def __len__(self):\n        photo_len = len(self.photo_files)\n        monet_len = len(self.monet_files)\n        if self.stage == 'train':\n            return min(photo_len,monet_len)\n        else:\n            return photo_len\n        \n    def __getitem__(self, idx):     \n        if self.stage == 'train':\n            monet_img = Image.open(self.monet_files[idx])\n            photo_img = Image.open(self.photo_files[idx])\n            monet_img = self.transform(monet_img,self.stage)\n            photo_img = self.transform(photo_img,self.stage)\n            return photo_img, monet_img\n        else:\n            photo_img = Image.open(self.photo_files[idx])\n            photo_img = self.transform(photo_img,self.stage)\n            return photo_img\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-29T18:17:46.959701Z","iopub.execute_input":"2023-04-29T18:17:46.960446Z","iopub.status.idle":"2023-04-29T18:17:46.968638Z","shell.execute_reply.started":"2023-04-29T18:17:46.960404Z","shell.execute_reply":"2023-04-29T18:17:46.967524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To load and iterate through datasets, we use `torch.utils.data.DataLoader`. To organize data processing, we create a datamodule using `pl.LightningDataModule`. Important methods include `setup` to create and transform datasets, `train_dataloader` to generate the training dataloader, and `test_dataloader` to generate the testing dataloader. Other methods can be found here. \n","metadata":{}},{"cell_type":"code","source":"class GANDataModule(pl.LightningDataModule):\n    def __init__(self, batch_size=8,transform=None):\n        super().__init__()\n        self.monet_files = sorted(glob.glob(\"/kaggle/input/gan-getting-started/monet_jpg/*.jpg\"))\n        self.photo_files = sorted(glob.glob(\"/kaggle/input/gan-getting-started/photo_jpg/*.jpg\"))\n        self.batch_size = batch_size\n        self.transform = transform\n        \n    def setup(self, stage):\n        if stage == \"train\":\n            self.train = GANDataset(monet_files=self.monet_files,photo_files=self.photo_files,\n                                    transform=self.transform,stage=stage)\n        else:\n            self.test = GANDataset(monet_files=self.monet_files,photo_files=self.photo_files,\n                                    transform=self.transform,stage=stage)\n            \n    def train_dataloader(self):\n        return DataLoader(self.train,batch_size=self.batch_size,shuffle=True)\n    \n    def test_dataloader(self):\n        return DataLoader(self.test, batch_size=self.batch_size,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T18:17:48.444382Z","iopub.execute_input":"2023-04-29T18:17:48.444764Z","iopub.status.idle":"2023-04-29T18:17:48.453277Z","shell.execute_reply.started":"2023-04-29T18:17:48.444730Z","shell.execute_reply":"2023-04-29T18:17:48.452028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below are some sample photos and Monet paintings. Our goal is to add Monet's style to the photos.","metadata":{}},{"cell_type":"code","source":"def display_img(img, nrow=4, title=\"\"):\n    img = img.detach().cpu()*0.5 + 0.5\n    img_tmp = make_grid(img, nrow=nrow).permute(1, 2, 0)\n    plt.figure(figsize=(18, 8))\n    plt.imshow(img_tmp)\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()\n\ndm = GANDataModule(batch_size=8,transform=ImgTransform(img_size=256))\ndm.setup(\"train\")\ndm.setup(\"test\")\n\ndataloader = dm.train_dataloader()\nphoto, monet = next(iter(dataloader))","metadata":{"execution":{"iopub.status.busy":"2023-04-29T18:18:06.788597Z","iopub.execute_input":"2023-04-29T18:18:06.789084Z","iopub.status.idle":"2023-04-29T18:18:07.007416Z","shell.execute_reply.started":"2023-04-29T18:18:06.789047Z","shell.execute_reply":"2023-04-29T18:18:07.006324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_img(monet,title='Monet Pictures')","metadata":{"execution":{"iopub.status.busy":"2023-04-29T18:18:08.184523Z","iopub.execute_input":"2023-04-29T18:18:08.185450Z","iopub.status.idle":"2023-04-29T18:18:09.133115Z","shell.execute_reply.started":"2023-04-29T18:18:08.185403Z","shell.execute_reply":"2023-04-29T18:18:09.132034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_img(photo,title='Photos')","metadata":{"execution":{"iopub.status.busy":"2023-04-29T18:18:11.445472Z","iopub.execute_input":"2023-04-29T18:18:11.446585Z","iopub.status.idle":"2023-04-29T18:18:12.192403Z","shell.execute_reply.started":"2023-04-29T18:18:11.446538Z","shell.execute_reply":"2023-04-29T18:18:12.189929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building","metadata":{"execution":{"iopub.status.busy":"2023-04-29T13:27:38.325546Z","iopub.execute_input":"2023-04-29T13:27:38.326280Z","iopub.status.idle":"2023-04-29T13:27:38.331034Z","shell.execute_reply.started":"2023-04-29T13:27:38.326236Z","shell.execute_reply":"2023-04-29T13:27:38.329699Z"}}},{"cell_type":"markdown","source":"### Generator","metadata":{}},{"cell_type":"markdown","source":"The CycleGAN generator is built using the `U-Net` architecture, which is designed with a U-shaped network consisting of downsampling and upsampling blocks with skip connections. This structure allows for the information to flow seamlessly between the encoder and decoder layers, allowing for better feature extraction and image reconstruction capabilities. By using the U-Net architecture, the CycleGAN generator is able to effectively generate high-quality images that are visually pleasing and accurate representations of the output domain.\n\nFor instance, the generator architecture looks like this:\n\n![U-Net architecture](https://www.researchgate.net/publication/342456648/figure/fig1/AS:906478919626752@1593132824738/U-Net-architecture-diagram-modified-from-the-original-study-27-Green-yellow-boxes_W640.jpg)","metadata":{}},{"cell_type":"code","source":"# Define the generator network - based on the U-net architecture\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        \n        def downsample(in_channels, out_channels, normalize=True):\n            layers = [nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_channels))\n            layers.append(nn.LeakyReLU(0.2))\n            return layers\n        \n        def upsample(in_channels, out_channels, dropout=False):\n            layers = [nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)]\n            layers.append(nn.InstanceNorm2d(out_channels))\n            if dropout:\n                layers.append(nn.Dropout(0.5))\n            layers.append(nn.ReLU())\n            return layers\n        \n        # Encoder layers (downsampling):\n        self.downsample_1 = nn.Sequential(*downsample(3, 64, normalize=False))\n        self.downsample_2 = nn.Sequential(*downsample(64, 128))\n        self.downsample_3 = nn.Sequential(*downsample(128, 256))\n        self.downsample_4 = nn.Sequential(*downsample(256, 512))\n        self.downsample_5 = nn.Sequential(*downsample(512, 512))\n        self.downsample_6 = nn.Sequential(*downsample(512, 512))\n        self.downsample_7 = nn.Sequential(*downsample(512, 512))\n        self.downsample_8 = nn.Sequential(*downsample(512, 512, normalize=False))\n        \n        # Decoder layers (upsampling):\n        self.upsample_1 = nn.Sequential(*upsample(512, 512, dropout=True))\n        self.upsample_2 = nn.Sequential(*upsample(1024, 512, dropout=True))\n        self.upsample_3 = nn.Sequential(*upsample(1024, 512, dropout=True))\n        self.upsample_4 = nn.Sequential(*upsample(1024, 512))\n        self.upsample_5 = nn.Sequential(*upsample(1024, 256))\n        self.upsample_6 = nn.Sequential(*upsample(512, 128))\n        self.upsample_7 = nn.Sequential(*upsample(256, 64))\n        self.upsample_8 = nn.Sequential(\n            nn.ConvTranspose2d(128, 3, 4, stride=2, padding=1),\n            nn.Tanh()\n        )\n    \n    def forward(self, x):\n        # Encoder pass (downsampling):\n        down_1 = self.downsample_1(x)\n        down_2 = self.downsample_2(down_1)\n        down_3 = self.downsample_3(down_2)\n        down_4 = self.downsample_4(down_3)\n        down_5 = self.downsample_5(down_4)\n        down_6 = self.downsample_6(down_5)\n        down_7 = self.downsample_7(down_6)\n        down_8 = self.downsample_8(down_7)\n        \n        # Decoder pass (upsampling):\n        up_1 = self.upsample_1(down_8)\n        up_2 = self.upsample_2(torch.cat([up_1, down_7], 1))\n        up_3 = self.upsample_3(torch.cat([up_2, down_6], 1))\n        up_4 = self.upsample_4(torch.cat([up_3, down_5], 1))\n        up_5 = self.upsample_5(torch.cat([up_4, down_4], 1))\n        up_6 = self.upsample_6(torch.cat([up_5, down_3], 1))\n        up_7 = self.upsample_7(torch.cat([up_6, down_2], 1))\n        up_8 = self.upsample_8(torch.cat([up_7, down_1], 1))\n        \n        return up_8\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-29T18:18:15.129176Z","iopub.execute_input":"2023-04-29T18:18:15.129559Z","iopub.status.idle":"2023-04-29T18:18:15.148514Z","shell.execute_reply.started":"2023-04-29T18:18:15.129523Z","shell.execute_reply":"2023-04-29T18:18:15.147105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## test generator\ngen_net = Generator()\nout = gen_net(photo)\ngen_net","metadata":{"execution":{"iopub.status.busy":"2023-04-29T18:18:15.747512Z","iopub.execute_input":"2023-04-29T18:18:15.747898Z","iopub.status.idle":"2023-04-29T18:18:18.201809Z","shell.execute_reply.started":"2023-04-29T18:18:15.747860Z","shell.execute_reply":"2023-04-29T18:18:18.200764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discriminator","metadata":{"execution":{"iopub.status.busy":"2023-04-29T15:25:12.395683Z","iopub.execute_input":"2023-04-29T15:25:12.396380Z","iopub.status.idle":"2023-04-29T15:25:12.403124Z","shell.execute_reply.started":"2023-04-29T15:25:12.396344Z","shell.execute_reply":"2023-04-29T15:25:12.399904Z"}}},{"cell_type":"markdown","source":"CycleGAN employs a unique approach to image discrimination by using the `PatchGAN` discriminator, which unlike other conventional networks, outputs a matrix of values instead of a single probability of the input image being real or fake. Each value of the output matrix corresponds to a specific portion of the input image, allowing for more precise evaluation. Values closer to 1 indicate real classification, while values closer to 0 indicate fake classification. This method provides improved discrimination capabilities, resulting in better quality and more accurate output images.\n\nFor instance, the discriminator architecture looks like this:\n\n![Discriminator architecture](https://www.researchgate.net/profile/Marija-Jegorova-2/publication/353016853/figure/fig2/AS:1042566166355968@1625578552453/Example-of-CycleGAN-architecture-Discriminator-PatchGAN-It-is-a-fully-convolutional_W640.jpg)\n","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, in_channels=3):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_channels, out_channels, stride, normalize):\n            layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_channels, affine=True, track_running_stats=True))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(in_channels, 64, stride=2, normalize=False),\n            *discriminator_block(64, 128, stride=2, normalize=True),\n            *discriminator_block(128, 256, stride=2, normalize=True),\n            *discriminator_block(256, 512, stride=1, normalize=True),\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-29T18:18:20.923187Z","iopub.execute_input":"2023-04-29T18:18:20.923950Z","iopub.status.idle":"2023-04-29T18:18:20.933207Z","shell.execute_reply.started":"2023-04-29T18:18:20.923909Z","shell.execute_reply":"2023-04-29T18:18:20.931795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## test discriminator\ndis_net = Discriminator()\nout = dis_net(photo)\ndis_net","metadata":{"execution":{"iopub.status.busy":"2023-04-29T18:18:21.456165Z","iopub.execute_input":"2023-04-29T18:18:21.456893Z","iopub.status.idle":"2023-04-29T18:18:22.126723Z","shell.execute_reply.started":"2023-04-29T18:18:21.456842Z","shell.execute_reply":"2023-04-29T18:18:22.125562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CycleGAN","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:28:12.368997Z","iopub.execute_input":"2023-04-29T16:28:12.370090Z","iopub.status.idle":"2023-04-29T16:28:12.374988Z","shell.execute_reply.started":"2023-04-29T16:28:12.370046Z","shell.execute_reply":"2023-04-29T16:28:12.373681Z"}}},{"cell_type":"markdown","source":"We build CycleGAN based on the above `Generator` and `Discriminator` network compenents:\n\n1. Photo2Monet Generator\n2. Monet2Photo Generator\n3. Monet Discriminator\n4. Photo Discriminator\n","metadata":{}},{"cell_type":"markdown","source":"![](https://1.bp.blogspot.com/-F7lW08tA2t0/X5424ebzBqI/AAAAAAAAKiA/zq24RKnPc5wwDvjqS7EUuXDzMPb_2rJaACLcBGAsYHQ/s806/Google%2BChromeScreenSnapz098.jpg)","metadata":{}},{"cell_type":"code","source":"class CycleGAN(pl.LightningModule):\n    def __init__(self, lr=2e-4, betas=(0.5, 0.999), lambda_w=10, display_epochs=30, photo_samples=photo):\n        super().__init__()\n        self.lr = lr\n        self.betas = betas\n        self.lambda_w = lambda_w\n        self.display_epochs = display_epochs\n        self.photo_samples = photo_samples\n        self.loss_history = []\n        self.epoch_count = 0\n        \n        self.gan_photo_monet = Generator()\n        self.gan_monet_photo = Generator()\n        self.dis_monet = Discriminator()\n        self.dis_photo = Discriminator()\n\n        \n    def forward(self, z):\n        return self.gan_photo_monet(z)\n    \n    def adv_criterion(self, y_hat, y):\n        return F.binary_cross_entropy_with_logits(y_hat, y)\n    \n    def recon_criterion(self, y_hat, y):\n        return F.l1_loss(y_hat, y)\n    \n    def adv_loss(self, real_X, disc_Y, gen_XY):\n        fake_Y = gen_XY(real_X)\n        disc_fake_Y_hat = disc_Y(fake_Y)\n        adv_loss_XY = self.adv_criterion(disc_fake_Y_hat, torch.ones_like(disc_fake_Y_hat))\n        return adv_loss_XY, fake_Y\n    \n    def id_loss(self, real_X, gen_YX):\n        id_X = gen_YX(real_X)\n        id_loss_X = self.recon_criterion(id_X, real_X)\n        return id_loss_X\n    \n    def cycle_loss(self, real_X, fake_Y, gen_YX):\n        cycle_X = gen_YX(fake_Y)\n        cycle_loss_X = self.recon_criterion(cycle_X, real_X)\n        return cycle_loss_X\n        \n    def gen_loss(self, real_X, real_Y, gen_XY, gen_YX, disc_Y):\n        adv_loss_XY, fake_Y = self.adv_loss(real_X, disc_Y, gen_XY)\n\n        id_loss_Y = self.id_loss(real_Y, gen_XY)\n        \n        cycle_loss_X = self.cycle_loss(real_X, fake_Y, gen_YX)\n        cycle_loss_Y = self.cycle_loss(real_Y, gen_YX(real_Y), gen_XY)\n        cycle_loss = cycle_loss_X + cycle_loss_Y\n        \n        gen_loss_XY = adv_loss_XY + 0.5*self.lambda_w*id_loss_Y + self.lambda_w*cycle_loss\n        return gen_loss_XY\n    \n    def disc_loss(self, real_X, fake_X, disc_X):\n        disc_fake_hat = disc_X(fake_X.detach())\n        disc_fake_loss = self.adv_criterion(disc_fake_hat, torch.zeros_like(disc_fake_hat))\n        \n        disc_real_hat = disc_X(real_X)\n        disc_real_loss = self.adv_criterion(disc_real_hat, torch.ones_like(disc_real_hat))\n        \n        disc_loss = (disc_fake_loss+disc_real_loss) / 2\n        return disc_loss\n    \n    def configure_optimizers(self):\n        params = {\n            \"lr\": self.lr,\n            \"betas\": self.betas,\n        }\n        opt_gan_photo_monet = torch.optim.Adam(self.gan_photo_monet.parameters(), **params)\n        opt_gan_monet_photo = torch.optim.Adam(self.gan_monet_photo.parameters(), **params)\n        \n        opt_dis_monet = torch.optim.Adam(self.dis_monet.parameters(), **params)\n        opt_dis_photo = torch.optim.Adam(self.dis_photo.parameters(), **params)\n        \n        return [opt_gan_photo_monet, opt_gan_monet_photo, opt_dis_monet, opt_dis_photo], []\n    \n    def training_step(self, batch, batch_idx, optimizer_idx):\n        real_M, real_P = batch\n        if optimizer_idx == 0:\n            gen_loss_PM = self.gen_loss(real_P, real_M, self.gan_photo_monet, self.gan_monet_photo, self.dis_monet)\n            return gen_loss_PM\n        if optimizer_idx == 1:\n            gen_loss_MP = self.gen_loss(real_M, real_P, self.gan_monet_photo, self.gan_photo_monet, self.dis_photo)\n            return gen_loss_MP\n        \n        if optimizer_idx == 2:\n            disc_loss_M = self.disc_loss(real_M, self.gan_photo_monet(real_P), self.dis_monet)\n            return disc_loss_M\n        if optimizer_idx == 3:\n            disc_loss_P = self.disc_loss(real_P, self.gan_monet_photo(real_M), self.dis_photo)\n            return disc_loss_P\n    \n    def training_epoch_end(self, outputs):\n        self.epoch_count += 1\n        \n        losses = []\n        for j in range(4):\n            loss = np.mean([out[j][\"loss\"].item() for out in outputs])\n            losses.append(loss)\n        self.loss_history.append(losses)\n        \n        if self.epoch_count%10 == 0:\n            print(\n                f\"Epoch {self.epoch_count} -\",\n                f\"gen_loss_PM: {losses[0]:.5f} -\",\n                f\"gen_loss_MP: {losses[1]:.5f} -\",\n                f\"disc_loss_M: {losses[2]:.5f} -\",\n                f\"disc_loss_P: {losses[3]:.5f}\",\n            )\n        \n        if self.epoch_count%self.display_epochs==0 or self.epoch_count==1:\n            gen_monets = self(self.photo_samples.to(self.device)).detach().cpu()\n            display_img(\n                torch.cat([self.photo_samples, gen_monets]),\n                nrow=4,\n                title=f\"Epoch {self.epoch_count}: Photo-to-Monet Translation\",\n            )\n            \n    def predict_step(self, batch, batch_idx):\n        return self(batch)\n    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-29T18:27:17.139195Z","iopub.execute_input":"2023-04-29T18:27:17.139767Z","iopub.status.idle":"2023-04-29T18:27:17.161639Z","shell.execute_reply.started":"2023-04-29T18:27:17.139728Z","shell.execute_reply":"2023-04-29T18:27:17.160245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CycleGAN()\nmodel","metadata":{"execution":{"iopub.status.busy":"2023-04-29T18:27:21.316809Z","iopub.execute_input":"2023-04-29T18:27:21.317551Z","iopub.status.idle":"2023-04-29T18:27:22.640860Z","shell.execute_reply.started":"2023-04-29T18:27:21.317514Z","shell.execute_reply":"2023-04-29T18:27:22.639771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Process","metadata":{}},{"cell_type":"code","source":"trainer = pl.Trainer(\n    accelerator=\"gpu\",\n    devices=1,\n    logger=False,\n    enable_checkpointing=False,\n    max_epochs=120,\n)\n\n\ndm = GANDataModule(batch_size=8,transform=ImgTransform(img_size=256))\ndm.setup(\"train\")\n\n\nmodel = CycleGAN()\ntrainer.fit(model, datamodule=dm)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T18:27:29.955878Z","iopub.execute_input":"2023-04-29T18:27:29.956597Z","iopub.status.idle":"2023-04-29T18:30:16.737065Z","shell.execute_reply.started":"2023-04-29T18:27:29.956559Z","shell.execute_reply":"2023-04-29T18:30:16.735708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = [\"gen_loss_photo2monet\", \"gen_loss_monet2photo\", \"disc_loss_monet\", \"disc_loss_photo\"]\ntitles = [\"Generator Loss Curves\", \"Discriminator Loss Curves\"]\nnum_epochs = len(model.loss_history)\nplt.figure(figsize=(18, 4.5))\nfor j in range(4):\n    if j%2 == 0:\n        plt.subplot(1, 2, (j//2)+1)\n        plt.title(titles[j//2])\n        plt.ylabel(\"Loss\")\n        plt.xlabel(\"Epoch\")\n    plt.plot(\n        np.arange(1, num_epochs+1),\n        [losses[j] for losses in model.loss_history],\n        label=labels[j],\n    )\n    plt.legend(loc=\"upper right\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-29T18:30:16.739887Z","iopub.execute_input":"2023-04-29T18:30:16.740381Z","iopub.status.idle":"2023-04-29T18:30:17.182891Z","shell.execute_reply.started":"2023-04-29T18:30:16.740338Z","shell.execute_reply":"2023-04-29T18:30:17.181812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T18:30:17.184444Z","iopub.execute_input":"2023-04-29T18:30:17.185210Z","iopub.status.idle":"2023-04-29T18:30:17.193397Z","shell.execute_reply.started":"2023-04-29T18:30:17.185163Z","shell.execute_reply":"2023-04-29T18:30:17.192189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"dm_test = GANDataModule(batch_size=8,transform=ImgTransform(img_size=256))\ndm_test.setup(\"test\")","metadata":{"execution":{"iopub.status.busy":"2023-04-29T18:30:17.196394Z","iopub.execute_input":"2023-04-29T18:30:17.197347Z","iopub.status.idle":"2023-04-29T18:30:17.227414Z","shell.execute_reply.started":"2023-04-29T18:30:17.197301Z","shell.execute_reply":"2023-04-29T18:30:17.226389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = trainer.predict(model, (dm_test.test_dataloader()))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-29T18:30:17.229049Z","iopub.execute_input":"2023-04-29T18:30:17.229436Z","iopub.status.idle":"2023-04-29T18:30:31.533201Z","shell.execute_reply.started":"2023-04-29T18:30:17.229398Z","shell.execute_reply":"2023-04-29T18:30:31.532139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" os.makedirs(\"../images\", exist_ok=True)\n\nidx = 0\nfor tensor in predictions:\n    for monet in tensor:\n\n        monet = monet.squeeze()\n        monet = monet * 0.5 + 0.5\n        monet = monet * 255\n        monet = monet.detach().cpu().numpy().astype(np.uint8)\n        \n        monet = np.transpose(monet, [1,2,0])\n        \n        monet = Image.fromarray(monet)\n        monet.save(f\"../images/{idx}.jpg\")\n        #save_image((monet.squeeze()*0.5+0.5), fp=f\"../images/{idx}.jpg\")\n        idx += 1\n\nshutil.make_archive(\"/kaggle/working/images\", \"zip\", \"/kaggle/images\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"In conclusion, our project successfully trained a GAN model using CycleGAN to generate realistic images in Monet's art style. Our custom dataset of photos and Monet's art pieces, along with various image preprocessing techniques, aided in producing high-quality images. Throughout the training process, we monitored the loss functions and evaluated the GAN model's performance on a validation dataset. The generated images demonstrate that the GAN model can emulate Monet's art style with a high level of fidelity, with room for further improvement. Future work can focus on enhancing the GAN model's performance, developing objective evaluation metrics, and investigating ways to apply the generated images to other use cases, such as art curation and creative design.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}